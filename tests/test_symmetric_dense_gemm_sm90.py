import torch
import pytest

from quack.symmetric_dense_gemm_sm90 import symmetric_dense_gemm

class TestSymmetricGemm:
    """Unit tests for symmetric dense GEMM wrapper."""
    
    @pytest.fixture(params=[torch.float16, torch.bfloat16, torch.float32])
    def dtype(self, request):
        """Test different data types."""
        return request.param
    
    @property
    def default_shape(self):
        """Default shape for most tests (L, M, K)."""
        return (2, 1024, 512)
    
    def torch_reference(self, a, b=None, c=None, alpha=1.0, beta=1.0):
        """Reference implementation using PyTorch operations.
        
        Args:
            a: Input tensor A of shape (L, M, K)
            b: Input tensor B of shape (L, M, K) - if None, uses A (symmetric case)
            c: Optional bias tensor C of shape (L, M, M)
            alpha: Scaling factor for A @ B^T
            beta: Scaling factor for C
            
        Returns:
            Result tensor of shape (L, M, M)
        """
        if b is None:
            b = a
            
        # Use einsum for batched matrix multiplication: A @ B^T
        # a: (L, M, K), b: (L, M, K) -> result: (L, M, M)
        result = alpha * torch.einsum('lmk,lnk->lmn', a, b)
        
        if c is not None:
            result = result + beta * c
                
        return result
    
    def create_test_tensor(self, L, M, K, dtype, device, stride_pattern="m_major", seed=None):
        """Create test tensor with specified stride pattern.
        
        Args:
            L, M, K: Tensor dimensions
            dtype: Data type
            device: Device ('cuda' or 'cpu')
            stride_pattern: How to arrange strides - 'm_major' means M has stride 1, 'k_major' means K has stride 1
            seed: Random seed for reproducibility
        """
        if stride_pattern == "m_major":
            # M has stride 1: (L, M, K) with strides (M*K, 1, M)
            tensor = torch.empty_strided(
                (L, M, K), 
                (M*K, 1, M), 
                dtype=dtype, 
                device=device
            )
        elif stride_pattern == "k_major":
            # K has stride 1: (L, M, K) with strides (M*K, K, 1)
            tensor = torch.empty_strided(
                (L, M, K), 
                (M*K, K, 1), 
                dtype=dtype, 
                device=device
            )
        else:
            raise ValueError(f"Unsupported stride pattern: {stride_pattern}")
            
        # Fill with random data
        if seed is not None:
            torch.manual_seed(seed)
        tensor.uniform_(-2, 2)
        return tensor
    
    def create_symmetric_tensor(self, L, M, dtype, device, seed=None):
        """Create a symmetric tensor of shape (L, M, M)."""
        if seed is not None:
            torch.manual_seed(seed)
            
        tensor = torch.randn(L, M, M, dtype=dtype, device=device)
        
        for l in range(L):
            matrix = tensor[l, :, :]
            tensor[l, :, :] = (matrix + matrix.T) / 2
            
        return tensor
    
    def test_basic_symmetric_gemm(self, dtype):
        """Test basic symmetric GEMM without bias."""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
            
        L, M, K = self.default_shape
        device = 'cuda'
        
        # Create input tensor A with stride 1 along M dimension
        a = self.create_test_tensor(L, M, K, dtype, device, "m_major", seed=42)
        
        print(f"a.shape = {a.shape}, a.stride = {a.stride()}")
        
        # Test symmetric case (B = A)
        result_quack = symmetric_dense_gemm(a, a)
        result_torch = self.torch_reference(a, a)
        
        assert result_quack.shape == result_torch.shape == (L, M, M)

        if dtype == torch.float32:
            torch.testing.assert_close(result_quack, result_torch, atol=1e-4, rtol=1e-4)
        else:  # float16, bfloat16
            torch.testing.assert_close(result_quack, result_torch, atol=1e-2, rtol=1e-2)
    
    def test_symmetric_gemm_with_bias(self, dtype):
        """Test symmetric GEMM with bias tensor C."""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
            
        L, M, K = self.default_shape
        device = 'cuda'
        
        # Create input tensors
        a = self.create_test_tensor(L, M, K, dtype, device, "m_major", seed=42)
        c = self.create_symmetric_tensor(L, M, dtype, device, seed=123)
        
        # Compute with our wrapper
        result_quack = symmetric_dense_gemm(a, a, c=c)
        
        # Compute reference
        result_torch = self.torch_reference(a, a, c=c)
        
        # Check shapes match
        assert result_quack.shape == result_torch.shape == (L, M, M)
        
        # Check values match
        if dtype == torch.float32:
            torch.testing.assert_close(result_quack, result_torch, atol=1e-4, rtol=1e-4)
        else:
            torch.testing.assert_close(result_quack, result_torch, atol=1e-2, rtol=1e-2)
    
    def test_alpha_beta_scaling(self, dtype):
        """Test alpha and beta scaling factors."""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
            
        L, M, K = self.default_shape
        device = 'cuda'
        alpha, beta = 2.5, 0.5
        
        # Create input tensors
        a = self.create_test_tensor(L, M, K, dtype, device, "m_major", seed=42)
        c = self.create_symmetric_tensor(L, M, dtype, device, seed=123)
        
        # Compute with our wrapper
        result_quack = symmetric_dense_gemm(a, a, c=c, alpha=alpha, beta=beta)
        
        # Compute reference
        result_torch = self.torch_reference(a, a, c=c, alpha=alpha, beta=beta)
        
        # Check values match
        if dtype == torch.float32:
            torch.testing.assert_close(result_quack, result_torch, atol=1e-4, rtol=1e-4)
        else:
            torch.testing.assert_close(result_quack, result_torch, atol=1e-2, rtol=1e-2)
    
    def test_symmetry_property(self, dtype):
        """Test that output is actually symmetric (D = D^T)."""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
            
        L, M, K = self.default_shape
        device = 'cuda'
        
        # Create input tensor
        a = self.create_test_tensor(L, M, K, dtype, device, "m_major", seed=42)
      
        # Compute symmetric GEMM
        result = symmetric_dense_gemm(a, a)
        
        # Check symmetry for each batch
        for l in range(L):
            matrix = result[l, :, :]
            torch.testing.assert_close(matrix, matrix.T, atol=1e-6, rtol=1e-6)
    
    def test_different_sizes(self):
        """Test various matrix sizes to ensure robustness."""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
            
        device = 'cuda'
        dtype = torch.float16
        
        test_sizes = [
            (3, 128, 128),  
            (5, 256, 256),  
            (5, 1024, 1024), 
            (3, 2048, 2048),  
            (1, 4096, 4096),
        ]
        
        for L, M, K in test_sizes:
            a = self.create_test_tensor(L, M, K, dtype, device, "m_major", seed=42)
    
            result = symmetric_dense_gemm(a, a)
            expected = self.torch_reference(a, a)
            
            assert result.shape == (L, M, M)
            torch.testing.assert_close(result, expected, atol=1e-2, rtol=1e-2)
            
            # Verify symmetry
            for l in range(L):
                matrix = result[l, :, :]
                torch.testing.assert_close(matrix, matrix.T, atol=1e-6, rtol=1e-6)
    
    def test_different_stride_patterns(self, dtype):
        """Test symmetric GEMM with different stride patterns (m_major vs k_major)."""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
            
        L, M, K = self.default_shape
        device = 'cuda'
        
        a_m_major = self.create_test_tensor(L, M, K, dtype, device, "m_major", seed=42)
        
        data_contiguous = a_m_major.contiguous()
        a_k_major = torch.empty_strided(
            (L, M, K), 
            (M*K, K, 1), 
            dtype=dtype, 
            device=device
        )
        a_k_major.copy_(data_contiguous)
        
        assert torch.equal(a_m_major, a_k_major), "Input tensors should have identical values"
        assert a_m_major.stride() != a_k_major.stride(), "Stride patterns should be different"
        
        result_m_major = symmetric_dense_gemm(a_m_major, a_m_major)
        result_k_major = symmetric_dense_gemm(a_k_major, a_k_major)
        
        assert result_m_major.shape == result_k_major.shape == (L, M, M)
        
        if dtype == torch.float32:
            torch.testing.assert_close(result_m_major, result_k_major, atol=1e-6, rtol=1e-6)
        else: 
            torch.testing.assert_close(result_m_major, result_k_major, atol=1e-4, rtol=1e-4)
        
        expected = self.torch_reference(a_m_major, a_m_major)
        
        if dtype == torch.float32:
            torch.testing.assert_close(result_m_major, expected, atol=1e-4, rtol=1e-4)
            torch.testing.assert_close(result_k_major, expected, atol=1e-4, rtol=1e-4)
        else:
            torch.testing.assert_close(result_m_major, expected, atol=1e-2, rtol=1e-2)
            torch.testing.assert_close(result_k_major, expected, atol=1e-2, rtol=1e-2)

def run_tests():
    """Run all tests manually (for debugging)."""
    test_class = TestSymmetricGemm()
    
    try:
        # Test basic functionality
        print("Testing basic symmetric GEMM...")
        test_class.test_basic_symmetric_gemm(torch.float16)
        print("‚úì Basic test passed")
        
        # Test with bias
        print("Testing with bias...")
        test_class.test_symmetric_gemm_with_bias(torch.float16)
        print("‚úì Bias test passed")
        
        # Test scaling
        print("Testing alpha/beta scaling...")
        test_class.test_alpha_beta_scaling(torch.float16)
        print("‚úì Scaling test passed")
        
        # Test symmetry
        print("Testing symmetry property...")
        test_class.test_symmetry_property(torch.float16)
        print("‚úì Symmetry test passed")

        # Test different sizes
        print("Testing different sizes...")
        test_class.test_different_sizes()
        print("‚úì Different sizes test passed")
        
        # Test different stride patterns
        print("Testing different stride patterns...")
        test_class.test_different_stride_patterns(torch.float16)
        print("‚úì Different stride patterns test passed")
        
        print("\nüéâ All tests passed!")
        
    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_tests()